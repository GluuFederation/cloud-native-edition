name: awseks
# This builds, packages and tests pygluu-kubernetes
on: [workflow_dispatch]

jobs:
  build:

    runs-on: ubuntu-latest
    strategy:
      max-parallel: 4
      matrix:
        python-version: [3.7]

    steps:
    - name: Checkout code
      uses: actions/checkout@master
    - name: Set up Python ${{ matrix.python-version }}
      uses: actions/setup-python@v1
      with:
        python-version: ${{ matrix.python-version }}

    - name: Install dependencies
      run: |
        sudo apt-get update
        sudo apt-get install unzip -y
        sudo python3 -m pip install --upgrade pip
        sudo pip3 install setuptools --upgrade
        sudo pip3 install pyOpenSSL --upgrade
        sudo pip3 install requests --upgrade

    - name: Lint with flake8
      run: |
        sudo pip3 install flake8
        # stop the build if there are Python syntax errors or undefined names
        sudo flake8 . --count --select=E9,F63,F7,F82 --show-source --statistics
        # exit-zero treats all errors as warnings. The GitHub editor is 127 chars wide
        sudo flake8 . --count --exit-zero --max-complexity=10 --max-line-length=127 --statistics

    - name: Build Zipapp
      run: |
        sudo apt-get update
        sudo apt-get install build-essential
        sudo pip3 install shiv
        mkdir /home/runner/work/test/
        cp -r ./* /home/runner/work/test/
        cd /home/runner/work/test/
        sudo make zipapp

    - name: Install AWS dependencies
      run: |
          cd /home/runner/work/test/
          curl "https://awscli.amazonaws.com/awscli-exe-linux-x86_64.zip" -o "awscliv2.zip"
          unzip awscliv2.zip
          sudo ./aws/install
          curl -o aws-iam-authenticator https://amazon-eks.s3.us-west-2.amazonaws.com/1.15.10/2020-02-22/bin/linux/amd64/aws-iam-authenticator
          chmod +x ./aws-iam-authenticator
          mkdir -p $HOME/bin && cp ./aws-iam-authenticator $HOME/bin/aws-iam-authenticator && export PATH=$PATH:$HOME/bin
          echo 'export PATH=$PATH:$HOME/bin' >> ~/.bashrc
          curl --silent --location "https://github.com/weaveworks/eksctl/releases/latest/download/eksctl_$(uname -s)_amd64.tar.gz" | tar xz -C /tmp
          sudo mv /tmp/eksctl /usr/local/bin
          curl -o kubectl https://amazon-eks.s3.us-west-2.amazonaws.com/1.15.10/2020-02-22/bin/linux/amd64/kubectl
          chmod +x ./kubectl
          mkdir -p $HOME/bin && cp ./kubectl $HOME/bin/kubectl && export PATH=$PATH:$HOME/bin
          echo 'export PATH=$PATH:$HOME/bin' >> ~/.bashrc
          aws configure set region ${{ secrets.aws_region }} --profile default
          aws configure set aws_access_key_id ${{ secrets.aws_access_key_id }} --profile default
          aws configure set aws_secret_access_key ${{ secrets.aws_secret_access_key }} --profile default
          aws configure set output json --profile default
          export PATH=~/.local/bin:$PATH
          export PATH=$HOME/.local/lib:$PATH
          export PATH=$HOME/bin:$PATH
          echo 'export PATH=$HOME/.local/bin:$PATH' >> ~/.profile
          echo 'export PATH=$HOME/bin:$PATH' >> ~/.profile
          echo 'export PATH=$HOME/.local/lib:$PATH' >> ~/.profile
          ssh-keygen -q -t rsa -N '' -f ~/.ssh/id_rsa 2>/dev/null <<< y >/dev/null
          curl -fsSL -o get_helm.sh https://raw.githubusercontent.com/helm/helm/master/scripts/get-helm-3
          chmod 700 get_helm.sh
          ./get_helm.sh
          sudo helm version

    - name: Create EKS cluster
      run: |
        cd /home/runner/work/test/
        RAND_STR=$(openssl rand -hex 4)
        EKS_NAME=automation-test-"$RAND_STR"
        echo "$EKS_NAME" > eks_cluster_name
        cat eks_cluster_name
        eksctl create cluster $EKS_NAME --version 1.17 --nodegroup-name standard-workers --node-type t2.xlarge --zones ${{ secrets.aws_region }}a,${{ secrets.aws_region }}b,${{ secrets.aws_region }}c --nodes 2 --region ${{ secrets.aws_region }} --node-ami auto --ssh-public-key "~/.ssh/id_rsa.pub"

    #- name: Install Istio
    #  run: |
    #      sudo curl -sL https://istio.io/downloadIstioctl | sh -
    #      export PATH=$PATH:$HOME/.istioctl/bin
    #      $HOME/.istioctl/bin/istioctl install --set profile=default

    - name: Create settings.json
      id: settings
      run: |
        cat <<EOF > /home/runner/work/test/settings.json
        {
          "CN_ACCEPT_LICENSE": "Y",
          "CN_TEST_ENVIRONMENT": "Y",
          "CN_VERSION": "5.0.0_dev",
          "CN_UPGRADE_TARGET_VERSION": "",
          "CN_NGINX_INGRESS_NAMESPACE": "ingress-nginx",
          "CN_HELM_RELEASE_NAME": "helm",
          "CN_NGINX_INGRESS_RELEASE_NAME": "ningress",
          "CN_POSTGRES_NAMESPACE": "postgres",
          "CN_POSTGRES_REPLICAS": 3,
          "CN_POSTGRES_URL": "postgres.postgres.svc.cluster.local",
          "CN_USE_ISTIO": "N",
          "CN_USE_ISTIO_INGRESS": "N",
          "CN_ISTIO_SYSTEM_NAMESPACE": "istio-system",
          "CN_NODES_IPS": [],
          "CN_NODES_ZONES": [],
          "CN_NODES_NAMES": [],
          "CN_NODE_SSH_KEY": "~/.ssh/id_rsa",
          "CN_HOST_EXT_IP": "",
          "CN_VERIFY_EXT_IP": "",
          "installer-settings.aws.lbType": "clb",
          "CN_USE_ARN": "N",
          "CN_ARN_AWS_IAM": "",
          "CN_LB_ADD": "",
          "CN_REDIS_URL": "",
          "CN_REDIS_TYPE": "",
          "CN_REDIS_PW": "",
          "CN_REDIS_USE_SSL": "false",
          "CN_DEPLOYMENT_ARCH": "eks",
          "CN_PERSISTENCE_BACKEND": "ldap",
          "CN_INSTALL_JACKRABBIT": "Y",
          "CN_JACKRABBIT_STORAGE_SIZE": "4Gi",
          "CN_JACKRABBIT_URL": "http://jackrabbit:8080",
          "CN_JACKRABBIT_ADMIN_ID": "admin",
          "CN_JACKRABBIT_ADMIN_PASSWORD": "admin",
          "CN_JACKRABBIT_CLUSTER": "Y",
          "CN_JACKRABBIT_PG_USER": "jackrabbit",
          "CN_JACKRABBIT_PG_PASSWORD": "jackrabbit",
          "CN_JACKRABBIT_DATABASE": "jackrabbit",
          "CN_INSTALL_COUCHBASE": "Y",
          "CN_COUCHBASE_NAMESPACE": "cbns",
          "CN_COUCHBASE_VOLUME_TYPE": "io1",
          "CN_COUCHBASE_CLUSTER_NAME": "cbgluu",
          "CN_COUCHBASE_URL": "cbgluu.cbns.svc.cluster.local",
          "CN_COUCHBASE_USER": "gluu",
          "CN_COUCHBASE_INDEX_NUM_REPLICA": "0",
          "CN_COUCHBASE_SUPERUSER": "admin",
          "CN_COUCHBASE_PASSWORD": "nTB5#|",
          "CN_COUCHBASE_SUPERUSER_PASSWORD": "nTB5#2",
          "CN_COUCHBASE_CRT": "",
          "CN_COUCHBASE_CN": "Couchbase CN",
          "CN_COUCHBASE_SUBJECT_ALT_NAME": "",
          "CN_COUCHBASE_CLUSTER_FILE_OVERRIDE": "N",
          "CN_COUCHBASE_USE_LOW_RESOURCES": "Y",
          "CN_COUCHBASE_DATA_NODES": "",
          "CN_COUCHBASE_QUERY_NODES": "",
          "CN_COUCHBASE_INDEX_NODES": "",
          "CN_COUCHBASE_SEARCH_EVENTING_ANALYTICS_NODES": "",
          "CN_COUCHBASE_GENERAL_STORAGE": "",
          "CN_COUCHBASE_DATA_STORAGE": "",
          "CN_COUCHBASE_INDEX_STORAGE": "",
          "CN_COUCHBASE_QUERY_STORAGE": "",
          "CN_COUCHBASE_ANALYTICS_STORAGE": "",
          "CN_COUCHBASE_INCR_BACKUP_SCHEDULE": "*/30 * * * *",
          "CN_COUCHBASE_FULL_BACKUP_SCHEDULE": "*/30 * * * *",
          "CN_COUCHBASE_BACKUP_RETENTION_TIME": "24h",
          "CN_COUCHBASE_BACKUP_STORAGE_SIZE": "5Gi",
          "CN_LDAP_BACKUP_SCHEDULE": "*/30 * * * *",
          "CN_NUMBER_OF_EXPECTED_USERS": "",
          "CN_EXPECTED_TRANSACTIONS_PER_SEC": "",
          "CN_USING_CODE_FLOW": "",
          "CN_USING_SCIM_FLOW": "",
          "CN_USING_RESOURCE_OWNER_PASSWORD_CRED_GRANT_FLOW": "",
          "CN_DEPLOY_MULTI_CLUSTER": "N",
          "CN_HYBRID_LDAP_HELD_DATA": "",
          "CN_LDAP_JACKRABBIT_VOLUME": "io1",
          "CN_APP_VOLUME_TYPE": 7,
          "CN_LDAP_STATIC_VOLUME_ID": "",
          "CN_LDAP_STATIC_DISK_URI": "",
          "CN_CACHE_TYPE": "NATIVE_PERSISTENCE",
          "CN_NAMESPACE": "gluu",
          "CN_FQDN": "demoexample.gluu.org",
          "CN_COUNTRY_CODE": "US",
          "CN_STATE": "TX",
          "CN_EMAIL": "support@gluu.org",
          "CN_CITY": "Austin",
          "CN_ORG_NAME": "Gluu",
          "CN_GMAIL_ACCOUNT": "",
          "CN_GOOGLE_NODE_HOME_DIR": "",
          "CN_IS_CN_FQDN_REGISTERED": "N",
          "CN_LDAP_PASSWORD": "nTB5#|",
          "CN_ADMIN_PASSWORD": "@4n,Js",
          "CLIENT_API_PW": "n8H0NKuGTXsn",
          "CN_CLIENT_API_APPLICATION_KEYSTORE_CN": "client-api",
          "CN_CLIENT_API_ADMIN_KEYSTORE_CN": "client-api",
          "CLIENT_API_STORAGE": "h2",
          "LDAP_STORAGE_SIZE": "4Gi",
          "AUTH_SERVER_REPLICAS": 1,
          "OXTRUST_REPLICAS": 1,
          "LDAP_REPLICAS": 1,
          "OXSHIBBOLETH_REPLICAS": 1,
          "OXPASSPORT_REPLICAS": 1,
          "CLIENT_API_REPLICAS": 1,
          "FIDO2_REPLICAS": 1,
          "SCIM_REPLICAS": 1,
          "CASA_REPLICAS": 1,
          "RADIUS_REPLICAS": 1,
          "ENABLE_CONFIG_API": "Y",
          "ENABLE_OXTRUST_API": "N",
          "ENABLE_OXTRUST_TEST_MODE": "N",
          "ENABLE_CACHE_REFRESH": "N",
          "ENABLE_CLIENT_API": "Y",
          "ENABLE_RADIUS": "Y",
          "ENABLE_OXPASSPORT": "Y",
          "ENABLE_OXSHIBBOLETH": "Y",
          "ENABLE_CASA": "Y",
          "AUTH_SERVER_KEYS_LIFE": 48,
          "ENABLE_AUTH_SERVER_KEY_ROTATE": "Y",
          "ENABLE_FIDO2": "Y",
          "ENABLE_SCIM": "Y",
          "ENABLE_OXTRUST_API_BOOLEAN": "true",
          "ENABLE_OXTRUST_TEST_MODE_BOOLEAN": "false",
          "ENABLE_RADIUS_BOOLEAN": "true",
          "ENABLE_OXPASSPORT_BOOLEAN": "true",
          "ENABLE_CASA_BOOLEAN": "true",
          "ENABLE_SAML_BOOLEAN": "true",
          "EDIT_IMAGE_NAMES_TAGS": "N",
          "CONFIRM_PARAMS": "Y"
        }
        EOF

    - name: Test Pygluu ldap helm
      id: test_kubernetes_ldap_helm
      run: |
          cd /home/runner/work/test/
          sudo ./pygluu-kubernetes.pyz helm-install
          kubectl wait -n gluu --for=condition=ready pod --timeout=900s -l statefulset.kubernetes.io/pod-name=helm-opendj-0 || sudo kubectl logs -l app=opendj -c opendj -n gluu || echo "Not Found"
          kubectl -n gluu wait --for=condition=available --timeout=600s deploy/helm-auth-server || kubectl logs -l app=auth-server -c auth-server -n gluu || echo "Not Found"
          kubectl -n gluu wait --for=condition=available --timeout=300s deploy/helm-oxpassport || kubectl logs -l app=oxpassport -c oxpassport -n gluu || echo "Not Found"
          kubectl -n gluu wait --for=condition=available --timeout=300s deploy/helm-casa || kubectl logs -l app=casa -c casa -n gluu || echo "Not Found"
          kubectl -n gluu wait --for=condition=available --timeout=300s deploy/helm-client-api || kubectl logs -l app=client-api -c client-api -n gluu || echo "Not Found"
          kubectl wait -n gluu --for=condition=ready pod --timeout=300s -l statefulset.kubernetes.io/pod-name=helm-oxshibboleth-0 || kubectl logs -l app=oxshiboleth -c oxshiboleth -n gluu || echo "Not Found"
          kubectl -n gluu wait --for=condition=available --timeout=300s deploy/helm-radius || kubectl logs -l app=radius -c radius -n gluu
          sudo helm delete helm-ldap-backup -n gluu || echo "Resources not found"
          sudo kubectl get po -n gluu
          sudo ./pygluu-kubernetes.pyz helm-uninstall
          sleep 20

    - name: Test Pygluu couchbase helm
      id: test_kubernetes_couchbase_helm
      run: |
        cd /home/runner/work/test/
        cat settings.json | sed 's/"CN_INSTALL_COUCHBASE": "N"/"CN_INSTALL_COUCHBASE": "Y"/g' > tmpfile.py && mv tmpfile.py settings.json
        sudo ./pygluu-kubernetes.pyz install-couchbase
        cat settings.json | sed 's/"CN_INSTALL_COUCHBASE": "Y"/"CN_INSTALL_COUCHBASE": "N"/g' | sed -s "s@cbgluu.cbns.svc.cluster.local@$COUCHBASE_IP@g" > tmpfile.py && mv tmpfile.py settings.json
        cat settings.json | sed 's/"CN_COUCHBASE_URL": "couchbase"/"CN_COUCHBASE_URL": "cbgluu.cbns.svc.cluster.local"/g' > tmpfile.py && mv tmpfile.py settings.json
        cat settings.json | sed 's/"CN_COUCHBASE_URL": ""/"CN_COUCHBASE_URL": "cbgluu.cbns.svc.cluster.local"/g' > tmpfile.py && mv tmpfile.py settings.json
        sudo ./pygluu-kubernetes.pyz helm-install
        kubectl -n gluu wait --for=condition=available --timeout=600s deploy/helm-auth-server || kubectl logs -l app=auth-server -c auth-server -n gluu || echo "Not Found"
        kubectl -n gluu wait --for=condition=available --timeout=300s deploy/helm-oxpassport || kubectl logs -l app=oxpassport -c oxpassport -n gluu || echo "Not Found"
        kubectl -n gluu wait --for=condition=available --timeout=300s deploy/helm-casa || kubectl logs -l app=casa -c casa -n gluu || echo "Not Found"
        kubectl -n gluu wait --for=condition=available --timeout=300s deploy/helm-client-api || kubectl logs -l app=client-api -c client-api -n gluu || echo "Not Found"
        kubectl wait -n gluu --for=condition=ready pod --timeout=300s -l statefulset.kubernetes.io/pod-name=helm-oxshibboleth-0 || kubectl logs -l app=oxshiboleth -c oxshiboleth -n gluu || echo "Not Found"
        kubectl -n gluu wait --for=condition=available --timeout=300s deploy/helm-radius || kubectl logs -l app=radius -c radius -n gluu
        kubectl get po -n cbns
        kubectl get po -n gluu
        kubectl delete -f couchbase-backup.yaml || echo "Resources not found"
        sudo ./pygluu-kubernetes.pyz uninstall-couchbase
        sudo ./pygluu-kubernetes.pyz helm-uninstall
        sleep 20
        kubectl get po -n cbns
        kubectl get po -n gluu


    - name: Uninstall and destroy cluster
      if: always()
      run: |
          cd /home/runner/work/test/
          sudo ./pygluu-kubernetes.pyz uninstall-couchbase || echo "pygluu failed"
          sudo ./pygluu-kubernetes.pyz helm-uninstall || echo "pygluu failed"
          EKS_NAME=$(cat eks_cluster_name || echo "noname")
          eksctl delete cluster $EKS_NAME
