name: awseks
# This builds, packages and tests pygluu-kubernetes
on:
  schedule:
    - cron:  '0 0 * * *'
jobs:
  build:

    runs-on: ubuntu-latest
    strategy:
      max-parallel: 4
      matrix:
        python-version: [3.7]

    steps:
    - name: Checkout code
      uses: actions/checkout@master
    - name: Set up Python ${{ matrix.python-version }}
      uses: actions/setup-python@v1
      with:
        python-version: ${{ matrix.python-version }}

    - name: Install dependencies
      run: |
        sudo apt-get update
        sudo apt-get install unzip -y
        sudo python3 -m pip install --upgrade pip
        sudo pip3 install setuptools --upgrade
        sudo pip3 install pyOpenSSL --upgrade
        sudo pip3 install requests --upgrade
        git clone --recursive https://github.com/kubernetes-client/python.git
        cd python
        git checkout release-11.0
        sed 's/raise ValueError("Invalid value for `conditions`, must not be `None`")/pass/g' ./kubernetes/client/models/v1beta1_custom_resource_definition_status.py > tmpfile.py && mv tmpfile.py ./kubernetes/client/models/v1beta1_custom_resource_definition_status.py
        sudo python3 setup.py install

    - name: Lint with flake8
      run: |
        sudo pip3 install flake8
        # stop the build if there are Python syntax errors or undefined names
        sudo flake8 . --count --select=E9,F63,F7,F82 --show-source --statistics
        # exit-zero treats all errors as warnings. The GitHub editor is 127 chars wide
        sudo flake8 . --count --exit-zero --max-complexity=10 --max-line-length=127 --statistics

    - name: Build Zipapp
      run: |
        sudo apt-get update
        sudo apt-get install build-essential
        sudo pip3 install shiv
        mkdir /home/runner/work/test/
        cp -r ./* /home/runner/work/test/
        cd /home/runner/work/test/
        sed '/kubernetes>=11.0.0b2/d' ./setup.py > tmpfile.py && mv tmpfile.py setup.py
        sudo make zipapp

    - name: Install AWS dependencies
      run: |
          cd /home/runner/work/test/
          curl "https://awscli.amazonaws.com/awscli-exe-linux-x86_64.zip" -o "awscliv2.zip"
          unzip awscliv2.zip
          sudo ./aws/install
          curl -o aws-iam-authenticator https://amazon-eks.s3.us-west-2.amazonaws.com/1.15.10/2020-02-22/bin/linux/amd64/aws-iam-authenticator
          chmod +x ./aws-iam-authenticator
          mkdir -p $HOME/bin && cp ./aws-iam-authenticator $HOME/bin/aws-iam-authenticator && export PATH=$PATH:$HOME/bin
          echo 'export PATH=$PATH:$HOME/bin' >> ~/.bashrc
          curl --silent --location "https://github.com/weaveworks/eksctl/releases/latest/download/eksctl_$(uname -s)_amd64.tar.gz" | tar xz -C /tmp
          sudo mv /tmp/eksctl /usr/local/bin
          curl -o kubectl https://amazon-eks.s3.us-west-2.amazonaws.com/1.15.10/2020-02-22/bin/linux/amd64/kubectl
          chmod +x ./kubectl
          mkdir -p $HOME/bin && cp ./kubectl $HOME/bin/kubectl && export PATH=$PATH:$HOME/bin
          echo 'export PATH=$PATH:$HOME/bin' >> ~/.bashrc
          aws configure set region ${{ secrets.aws_region }} --profile default
          aws configure set aws_access_key_id ${{ secrets.aws_access_key_id }} --profile default
          aws configure set aws_secret_access_key ${{ secrets.aws_secret_access_key }} --profile default
          aws configure set output json --profile default
          export PATH=~/.local/bin:$PATH
          export PATH=$HOME/.local/lib:$PATH
          export PATH=$HOME/bin:$PATH
          echo 'export PATH=$HOME/.local/bin:$PATH' >> ~/.profile
          echo 'export PATH=$HOME/bin:$PATH' >> ~/.profile
          echo 'export PATH=$HOME/.local/lib:$PATH' >> ~/.profile
          ssh-keygen -q -t rsa -N '' -f ~/.ssh/id_rsa 2>/dev/null <<< y >/dev/null
          curl -fsSL -o get_helm.sh https://raw.githubusercontent.com/helm/helm/master/scripts/get-helm-3
          chmod 700 get_helm.sh
          ./get_helm.sh
          sudo helm version

    - name: Create EKS cluster
      run: |
        cd /home/runner/work/test/
        RAND_STR=$(openssl rand -hex 4)
        EKS_NAME=automation-test-"$RAND_STR"
        echo "$EKS_NAME" > eks_cluster_name
        cat eks_cluster_name
        eksctl create cluster $EKS_NAME --version 1.15 --nodegroup-name standard-workers --node-type t2.xlarge --zones ${{ secrets.aws_region }}a,${{ secrets.aws_region }}b,${{ secrets.aws_region }}c --nodes 2 --region ${{ secrets.aws_region }} --node-ami auto --ssh-public-key "~/.ssh/id_rsa.pub"

    - name: Create settings.json
      id: settings
      run: |
        cat <<EOF > /home/runner/work/test/settings.json
        {
          "ACCEPT_GLUU_LICENSE": "Y",
          "GLUU_VERSION": "4.1",
          "GLUU_UPGRADE_TARGET_VERSION": "",
          "NGINX_INGRESS_NAMESPACE": "ingress-nginx",
          "GLUU_HELM_RELEASE_NAME": "helm",
          "NGINX_INGRESS_RELEASE_NAME": "ningress",
          "NODES_IPS": [],
          "NODES_ZONES": [],
          "NODES_NAMES": [],
          "NODE_SSH_KEY": "~/.ssh/id_rsa",
          "HOST_EXT_IP": "",
          "VERIFY_EXT_IP": "",
          "AWS_LB_TYPE": "clb",
          "USE_ARN": "N",
          "ARN_AWS_IAM": "",
          "LB_ADD": "",
          "REDIS_URL": "",
          "REDIS_TYPE": "",
          "REDIS_PW": "",
          "REDIS_USE_SSL": "false",
          "DEPLOYMENT_ARCH": "eks",
          "PERSISTENCE_BACKEND": "ldap",
          "INSTALL_COUCHBASE": "Y",
          "COUCHBASE_NAMESPACE": "cbns",
          "COUCHBASE_VOLUME_TYPE": "io1",
          "COUCHBASE_CLUSTER_NAME": "cbgluu",
          "COUCHBASE_FQDN": "*.cbns.svc.cluster.local",
          "COUCHBASE_URL": "cbgluu.cbns.svc.cluster.local",
          "COUCHBASE_USER": "admin",
          "COUCHBASE_PASSWORD": "nTB5#|",
          "COUCHBASE_CRT": "",
          "COUCHBASE_CN": "Couchbase CN",
          "COUCHBASE_SUBJECT_ALT_NAME": "",
          "COUCHBASE_CLUSTER_FILE_OVERRIDE": "N",
          "COUCHBASE_USE_LOW_RESOURCES": "Y",
          "COUCHBASE_DATA_NODES": "",
          "COUCHBASE_QUERY_NODES": "",
          "COUCHBASE_INDEX_NODES": "",
          "COUCHBASE_SEARCH_EVENTING_ANALYTICS_NODES": "",
          "COUCHBASE_GENERAL_STORAGE": "",
          "COUCHBASE_DATA_STORAGE": "",
          "COUCHBASE_INDEX_STORAGE": "",
          "COUCHBASE_QUERY_STORAGE": "",
          "COUCHBASE_ANALYTICS_STORAGE": "",
          "COUCHBASE_BACKUP_SCHEDULE": "*/30 * * * *",
          "COUCHBASE_BACKUP_RESTORE_POINTS": 1,
          "LDAP_BACKUP_SCHEDULE": "*/30 * * * *",
          "NUMBER_OF_EXPECTED_USERS": "",
          "EXPECTED_TRANSACTIONS_PER_SEC": "",
          "USING_CODE_FLOW": "",
          "USING_SCIM_FLOW": "",
          "USING_RESOURCE_OWNER_PASSWORD_CRED_GRANT_FLOW": "",
          "DEPLOY_MULTI_CLUSTER": "N",
          "HYBRID_LDAP_HELD_DATA": "",
          "LDAP_VOLUME": "io1",
          "LDAP_VOLUME_TYPE": 7,
          "LDAP_STATIC_VOLUME_ID": "",
          "LDAP_STATIC_DISK_URI": "",
          "GLUU_CACHE_TYPE": "NATIVE_PERSISTENCE",
          "GLUU_NAMESPACE": "gluu",
          "GLUU_FQDN": "demoexample.gluu.org",
          "COUNTRY_CODE": "US",
          "STATE": "TX",
          "EMAIL": "support@gluu.org",
          "CITY": "Austin",
          "ORG_NAME": "Gluu",
          "GMAIL_ACCOUNT": "",
          "GOOGLE_NODE_HOME_DIR": "",
          "IS_GLUU_FQDN_REGISTERED": "N",
          "LDAP_PW": "nTB5#|",
          "ADMIN_PW": "@4n,Js",
          "OXD_SERVER_PW": "n8H0NKuGTXsn",
          "OXD_APPLICATION_KEYSTORE_CN": "oxd-server",
          "OXD_ADMIN_KEYSTORE_CN": "oxd-server",
          "LDAP_STORAGE_SIZE": "4Gi",
          "OXAUTH_REPLICAS": 1,
          "OXTRUST_REPLICAS": 1,
          "LDAP_REPLICAS": 1,
          "OXSHIBBOLETH_REPLICAS": 1,
          "OXPASSPORT_REPLICAS": 1,
          "OXD_SERVER_REPLICAS": 1,
          "CASA_REPLICAS": 1,
          "RADIUS_REPLICAS": 1,
          "ENABLE_OXTRUST_API": "N",
          "ENABLE_OXTRUST_TEST_MODE": "N",
          "ENABLE_CACHE_REFRESH": "N",
          "ENABLE_OXD": "Y",
          "ENABLE_RADIUS": "Y",
          "ENABLE_OXPASSPORT": "Y",
          "ENABLE_OXSHIBBOLETH": "Y",
          "ENABLE_CASA": "Y",
          "ENABLE_KEY_ROTATE": "Y",
          "ENABLE_OXTRUST_API_BOOLEAN": "true",
          "ENABLE_OXTRUST_TEST_MODE_BOOLEAN": "false",
          "ENABLE_RADIUS_BOOLEAN": "true",
          "ENABLE_OXPASSPORT_BOOLEAN": "true",
          "ENABLE_CASA_BOOLEAN": "true",
          "ENABLE_SAML_BOOLEAN": "true",
          "EDIT_IMAGE_NAMES_TAGS": "N",
          "CONFIRM_PARAMS": "Y"
        }
        EOF

    - name: Test Pygluu ldap kustomize
      id: test_kubernetes_ldap_kustomize
      run: |
          cd /home/runner/work/test/
          sudo ./pygluu-kubernetes.pyz install-no-wait
          sleep 30
          kubectl wait -n gluu --for=condition=ready pod --timeout=900s -l statefulset.kubernetes.io/pod-name=opendj-0 || sudo kubectl logs -l app=opendj -n gluu || echo "Not Found"
          kubectl -n gluu wait --for=condition=available --timeout=600s deploy/oxauth || kubectl logs -l app=oxauth -n gluu
          kubectl wait -n gluu --for=condition=ready pod --timeout=300s -l statefulset.kubernetes.io/pod-name=oxtrust-0 || kubectl logs -l app=oxtrust -n gluu
          kubectl -n gluu wait --for=condition=available --timeout=300s deploy/oxpassport || kubectl logs -l app=oxpassport -n gluu
          kubectl -n gluu wait --for=condition=available --timeout=300s deploy/casa || kubectl logs -l app=casa -n gluu
          kubectl -n gluu wait --for=condition=available --timeout=300s deploy/oxd-server || kubectl logs -l app=oxd-server -n gluu
          kubectl wait -n gluu --for=condition=ready pod --timeout=300s -l statefulset.kubernetes.io/pod-name=oxshibboleth-0 || kubectl logs -l app=oxshiboleth -n gluu
          kubectl -n gluu wait --for=condition=available --timeout=300s deploy/radius || kubectl logs -l app=radius -n gluu
          sudo kubectl get po -n gluu
          sudo kubectl delete -f ldap-backup.yaml || echo "Resources not found"
          sudo ./pygluu-kubernetes.pyz uninstall
          sleep 20

    - name: Test Pygluu ldap helm
      id: test_kubernetes_ldap_helm
      run: |
          cd /home/runner/work/test/
          sudo ./pygluu-kubernetes.pyz helm-install
          sudo kubectl wait -n gluu --for=condition=ready pod --timeout=900s -l statefulset.kubernetes.io/pod-name=helm-opendj-0 || sudo kubectl logs -l app=opendj -n gluu || echo "Not Found"
          kubectl -n gluu wait --for=condition=available --timeout=600s deploy/helm-oxauth || kubectl logs -l app=oxauth -n gluu
          kubectl wait -n gluu --for=condition=ready pod --timeout=300s -l statefulset.kubernetes.io/pod-name=helm-oxtrust-0 || kubectl logs -l app=oxtrust -n gluu
          kubectl -n gluu wait --for=condition=available --timeout=300s deploy/helm-oxpassport || kubectl logs -l app=oxpassport -n gluu
          kubectl -n gluu wait --for=condition=available --timeout=300s deploy/helm-casa || kubectl logs -l app=casa -n gluu
          kubectl -n gluu wait --for=condition=available --timeout=300s deploy/helm-oxd-server || kubectl logs -l app=oxd-server -n gluu
          kubectl wait -n gluu --for=condition=ready pod --timeout=300s -l statefulset.kubernetes.io/pod-name=oxshibboleth-0 || kubectl logs -l app=oxshiboleth -n gluu
          kubectl -n gluu wait --for=condition=available --timeout=300s deploy/helm-radius || kubectl logs -l app=radius -n gluu
          sudo helm delete helm-ldap-backup -n gluu || echo "Resources not found"
          sudo kubectl get po -n gluu
          sudo ./pygluu-kubernetes.pyz helm-uninstall
          sleep 20

    - name: Test Pygluu couchbase kustomize
      id: test_kubernetes_couchbase_kustomize
      run: |
        cd /home/runner/work/test/
        wget ${{ secrets.cbpackage }}
        cat settings.json | sed 's/"PERSISTENCE_BACKEND": "ldap"/"PERSISTENCE_BACKEND": "couchbase"/g'  > tmpfile.py && mv tmpfile.py settings.json
        sudo ./pygluu-kubernetes.pyz install-couchbase
        cat settings.json | sed 's/"INSTALL_COUCHBASE": "Y"/"INSTALL_COUCHBASE": "N"/g' > tmpfile.py && mv tmpfile.py settings.json
        cat settings.json | sed 's/"COUCHBASE_URL": "couchbase"/"COUCHBASE_URL": "cbgluu.cbns.svc.cluster.local"/g' > tmpfile.py && mv tmpfile.py settings.json
        sudo ./pygluu-kubernetes.pyz install-no-wait
        kubectl -n gluu wait --for=condition=available --timeout=900s deploy/oxauth || kubectl logs -l app=oxauth -n gluu
        kubectl wait -n gluu --for=condition=ready pod --timeout=300s -l statefulset.kubernetes.io/pod-name=oxtrust-0 || kubectl logs -l app=oxtrust -n gluu
        kubectl -n gluu wait --for=condition=available --timeout=300s deploy/oxpassport || kubectl logs -l app=oxpassport -n gluu
        kubectl -n gluu wait --for=condition=available --timeout=300s deploy/casa || kubectl logs -l app=casa -n gluu
        kubectl -n gluu wait --for=condition=available --timeout=300s deploy/oxd-server || kubectl logs -l app=oxd-server -n gluu
        kubectl wait -n gluu --for=condition=ready pod --timeout=300s -l statefulset.kubernetes.io/pod-name=oxshibboleth-0 || kubectl logs -l app=oxshiboleth -n gluu
        kubectl -n gluu wait --for=condition=available --timeout=300s deploy/radius || kubectl logs -l app=radius -n gluu
        kubectl get po -n cbns
        kubectl get po -n gluu
        sudo ./pygluu-kubernetes.pyz uninstall-couchbase
        sudo ./pygluu-kubernetes.pyz uninstall
        kubectl delete -f couchbase-backup.yaml || echo "Resources not found"
        sleep 20
        sudo kubectl get po -n cbns
        sudo kubectl get po -n gluu

    - name: Test Pygluu couchbase helm
      id: test_kubernetes_couchbase_helm
      run: |
        cd /home/runner/work/test/
        cat settings.json | sed 's/"INSTALL_COUCHBASE": "N"/"INSTALL_COUCHBASE": "Y"/g' > tmpfile.py && mv tmpfile.py settings.json
        sudo ./pygluu-kubernetes.pyz install-couchbase
        cat settings.json | sed 's/"INSTALL_COUCHBASE": "Y"/"INSTALL_COUCHBASE": "N"/g' | sed -s "s@cbgluu.cbns.svc.cluster.local@$COUCHBASE_IP@g" > tmpfile.py && mv tmpfile.py settings.json
        cat settings.json | sed 's/"COUCHBASE_URL": "couchbase"/"COUCHBASE_URL": "cbgluu.cbns.svc.cluster.local"/g' > tmpfile.py && mv tmpfile.py settings.json
        cat settings.json | sed 's/"COUCHBASE_URL": ""/"COUCHBASE_URL": "cbgluu.cbns.svc.cluster.local"/g' > tmpfile.py && mv tmpfile.py settings.json
        sudo ./pygluu-kubernetes.pyz helm-install
        kubectl -n gluu wait --for=condition=available --timeout=900s deploy/helm-oxauth || kubectl logs -l app=oxauth -n gluu
        kubectl wait -n gluu --for=condition=ready pod --timeout=300s -l statefulset.kubernetes.io/pod-name=helm-oxtrust-0 || kubectl logs -l app=oxtrust -n gluu
        kubectl -n gluu wait --for=condition=available --timeout=300s deploy/helm-oxpassport || kubectl logs -l app=oxpassport -n gluu
        kubectl -n gluu wait --for=condition=available --timeout=300s deploy/helm-casa || kubectl logs -l app=casa -n gluu
        kubectl -n gluu wait --for=condition=available --timeout=300s deploy/helm-oxd-server || kubectl logs -l app=oxd-server -n gluu
        kubectl wait -n gluu --for=condition=ready pod --timeout=300s -l statefulset.kubernetes.io/pod-name=helm-oxshibboleth-0 || kubectl logs -l app=oxshiboleth -n gluu
        kubectl -n gluu wait --for=condition=available --timeout=300s deploy/helm-radius || kubectl logs -l app=radius -n gluu
        kubectl get po -n cbns
        kubectl get po -n gluu
        kubectl delete -f couchbase-backup.yaml || echo "Resources not found"
        sudo ./pygluu-kubernetes.pyz uninstall-couchbase
        sudo ./pygluu-kubernetes.pyz helm-uninstall
        sleep 20
        kubectl get po -n cbns
        kubectl get po -n gluu


    - name: Uninstall and destroy cluster
      if: always()
      run: |
          cd /home/runner/work/test/
          sudo ./pygluu-kubernetes.pyz uninstall-couchbase || echo "pygluu failed"
          sudo ./pygluu-kubernetes.pyz helm-uninstall || echo "pygluu failed"
          EKS_NAME=$(cat eks_cluster_name || echo "noname")
          eksctl delete cluster $EKS_NAME